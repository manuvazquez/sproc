{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> Main functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import sys\n",
    "import argparse\n",
    "import pathlib\n",
    "import datetime\n",
    "\n",
    "import yaml\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sproc.extend\n",
    "import sproc.hier\n",
    "import sproc.assemble\n",
    "import sproc.bundle\n",
    "import sproc.postprocess\n",
    "import sproc.structure\n",
    "import sproc.download\n",
    "import sproc.parse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Directory where the zip files are stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = pathlib.Path.cwd().parent / 'samples'\n",
    "assert directory.exists()\n",
    "directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing a single zip file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def cli_process_zip(args: list = None) -> None:\n",
    "    \n",
    "    parser = argparse.ArgumentParser(description='Process zip file')\n",
    "\n",
    "    parser.add_argument('zip_file', type=argparse.FileType('r'), help='zip file')\n",
    "    parser.add_argument('output_file', help='Output (parquet) file')\n",
    "\n",
    "    command_line_arguments = parser.parse_args(args)\n",
    "    \n",
    "    output_file = pathlib.Path(command_line_arguments.output_file)\n",
    "    assert output_file.suffix == '.parquet', 'a .parquet file was expected'\n",
    "    \n",
    "    data_df, deleted_series = sproc.assemble.distilled_data_from_zip(command_line_arguments.zip_file.name)\n",
    "    \n",
    "    res = sproc.assemble.merge_deleted(data_df, deleted_series)\n",
    "    res = sproc.assemble.parquet_amenable(res)\n",
    "    \n",
    "    res.to_parquet(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_file = directory /'yearly' / 'PlataformasAgregadasSinMenores_2018.zip'\n",
    "assert zip_file.exists()\n",
    "print(f'{zip_file=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = directory / 'year_2018.parquet'\n",
    "print(f'{output_file=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = [zip_file.as_posix(), output_file.as_posix()]\n",
    "cli_process_zip(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls {directory}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_parquet(output_file).head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending historical data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function to extend an existing *parquet* file with new data in a *zip* file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def cli_extend_parquet_with_zip(args: list = None) -> None:\n",
    "    \n",
    "    parser = argparse.ArgumentParser(description='Extend existing parquet file with data from a given zip')\n",
    "\n",
    "    parser.add_argument('history_file', type=argparse.FileType('r'), help='Parquet file')\n",
    "    parser.add_argument('zip_file', type=argparse.FileType('r'), help='Zip file')\n",
    "    parser.add_argument('output_file', help='Output (parquet) file')\n",
    "\n",
    "    command_line_arguments = parser.parse_args(args)\n",
    "    \n",
    "    history_file = pathlib.Path(command_line_arguments.history_file.name)\n",
    "    zip_file = pathlib.Path(command_line_arguments.zip_file.name)\n",
    "    \n",
    "    output_file = pathlib.Path(command_line_arguments.output_file)\n",
    "    assert output_file.suffix == '.parquet', 'a .parquet file was expected'\n",
    "    \n",
    "    sproc.extend.parquet_with_zip(history_file, zip_file, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing with some sample files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_file = directory /'2018-2021_20samples.parquet'\n",
    "assert history_file.exists()\n",
    "print(f'{history_file=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_zip_file = directory / 'PlataformasAgregadasSinMenores_202201_28-29.zip'\n",
    "assert new_zip_file.exists()\n",
    "print(f'{new_zip_file=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = directory / 'extended_sample.parquet'\n",
    "output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = [history_file.as_posix(), new_zip_file.as_posix(), output_file.as_posix()]\n",
    "cli_extend_parquet_with_zip(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls {directory}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Renaming columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def cli_rename_columns(args: list = None) -> None:\n",
    "    \n",
    "    parser = argparse.ArgumentParser(description='Rename columns')\n",
    "\n",
    "    parser.add_argument('hierarchical_file', type=argparse.FileType('r'), help='(Hierarchical) Parquet file')\n",
    "    parser.add_argument('mapping_file', type=argparse.FileType('r'), help='YAML file mapping hierarchical colum names to plain ones')\n",
    "    parser.add_argument('output_file', help='Output (parquet) file')\n",
    "\n",
    "    command_line_arguments = parser.parse_args(args)\n",
    "    \n",
    "    hierarchical_file = pathlib.Path(command_line_arguments.hierarchical_file.name)\n",
    "    assert hierarchical_file.suffix == '.parquet', 'a (hierarchical) .parquet file was expected'\n",
    "    \n",
    "    mapping_file = pathlib.Path(command_line_arguments.mapping_file.name)\n",
    "    assert (mapping_file.suffix == '.yaml') or (mapping_file.suffix == '.YAML'), 'a YAML file was expected'\n",
    "    \n",
    "    output_file = pathlib.Path(command_line_arguments.output_file)\n",
    "    assert output_file.suffix == '.parquet', 'a .parquet file was expected'\n",
    "    \n",
    "    with mapping_file.open() as yaml_data:\n",
    "        data_scheme = yaml.load(yaml_data, Loader=yaml.FullLoader)\n",
    "        \n",
    "    df = pd.read_parquet(hierarchical_file)\n",
    "    renamed_cols_df = sproc.hier.flatten_columns_names(df, data_scheme)\n",
    "    \n",
    "    renamed_cols_df.to_parquet(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "renamed_cols_output_file = directory / 'renamed_cols_extended_sample.parquet'\n",
    "renamed_cols_output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_file = directory / 'PLACE.yaml'\n",
    "assert mapping_file.exists()\n",
    "mapping_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = [output_file.as_posix(), mapping_file.as_posix(), renamed_cols_output_file.as_posix()]\n",
    "cli_rename_columns(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_parquet(renamed_cols_output_file).head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading a bunch of zip files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It receives a `list` of *zip* files and returns a (column-hierarchical) `pd.DataFrame` encompassing all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def read_zips(\n",
    "    files: list[str | pathlib.Path] # Input files\n",
    "    ) -> pd.DataFrame: # Procurement data\n",
    "    \"Build a `DataFrame` out of a bunch of zip files\"\n",
    "    \n",
    "    # at the beginning it is guaranteed that every file is present\n",
    "    for f in files:\n",
    "        \n",
    "        # in case `str` (rather than `Pathlib`s) were passed\n",
    "        f = pathlib.Path(f)\n",
    "        \n",
    "        assert f.exists(), f'{f} doesn\\'t exist'\n",
    "    \n",
    "    # accumulators for the data itself (contracts) and records of deleted entries\n",
    "    res_df = None\n",
    "    res_deleted_series = None\n",
    "\n",
    "    for f in tqdm(files, desc='Assembling files'):\n",
    "    # for f in files:\n",
    "\n",
    "        # print(f'Processing \"{f}\"')\n",
    "        tqdm.write(f'Processing \"{f}\"')\n",
    "\n",
    "        # data is read from the above *zip* file, and `concatenate`d into a single `pd.DataFrame`...\n",
    "        df = sproc.bundle.read_zip(f, concatenate=True)\n",
    "\n",
    "        # ...which is re-structured with multiindexed columns\n",
    "        df = sproc.hier.flat_df_to_multiindexed_df(df)\n",
    "\n",
    "        # every ATOM inside the zip file also contains information (at the beginning) about deleted entries\n",
    "        deleted_series = sproc.bundle.read_deleted_zip(f)\n",
    "\n",
    "        # if this is NOT the first iteration...\n",
    "        if res_df is not None:\n",
    "\n",
    "            # ...the new data is stacked\n",
    "            res_df = sproc.assemble.stack(res_df, df)\n",
    "            res_deleted_series = pd.concat((res_deleted_series, deleted_series), axis=0)\n",
    "\n",
    "        # ...if this is the first iteration\n",
    "        else:\n",
    "\n",
    "            # ...the new data is set as the accumulated result\n",
    "            res_df = df\n",
    "            res_deleted_series = deleted_series\n",
    "            \n",
    "    # some contracts show up more than once, and only the last update is to be kept\n",
    "    res_last_update_only_df = sproc.postprocess.keep_updates_only(res_df)\n",
    "\n",
    "    # a new *deleted* `pd.Series` is built by dropping duplicates (again, only the last one is kept)\n",
    "    deduplicated_deleted_series = sproc.postprocess.deduplicate_deleted_series(res_deleted_series)\n",
    "\n",
    "    # the *deleted* series is used to flag the appropriate entries in the \"main\" `pd.DataFrame`;\n",
    "    # the result is \"stateful\" in the sense that we know the state of each entry (deleted -and, if so, when- or not)\n",
    "    stateful_df = sproc.assemble.merge_deleted(res_last_update_only_df, deduplicated_deleted_series)\n",
    "    \n",
    "    # the number of filled-in rows for column `deleted_on` should match the number of `id`s in `deduplicated_deleted_series` that show up in `stateful_df`\n",
    "    assert stateful_df['deleted_on'].notna().sum() == len(set(stateful_df['id']) & set(deduplicated_deleted_series.index.get_level_values(2)))\n",
    "            \n",
    "    return stateful_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us pick a couple of files for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_files = ['PlataformasAgregadasSinMenores_2018.zip', 'PlataformasAgregadasSinMenores_2019.zip']\n",
    "zip_files = [directory/ 'yearly' / e for e in zip_files]\n",
    "zip_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_zips(zip_files)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A companion function to allow using the above from the command-line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def cli_read_zips(args: list = None) -> None:\n",
    "    \n",
    "    parser = argparse.ArgumentParser(description='Process a bunch of zip files')\n",
    "\n",
    "    parser.add_argument('input_files', type=argparse.FileType('r'), nargs='+', help='zip files')\n",
    "    parser.add_argument('-o', '--output_file', default='out.parquet', help='Output (parquet) file')\n",
    "\n",
    "    command_line_arguments = parser.parse_args(args)\n",
    "    \n",
    "    output_file = pathlib.Path(command_line_arguments.output_file)\n",
    "    assert output_file.suffix == '.parquet', 'a .parquet file was expected'\n",
    "        \n",
    "    # the `pd.DataFrame` is built...\n",
    "    df = read_zips([f.name for f in command_line_arguments.input_files])\n",
    "    \n",
    "    # ...rearranged for saving in parquet format\n",
    "    parquet_df = sproc.assemble.parquet_amenable(df)\n",
    "    \n",
    "    parquet_df.to_parquet(output_file)\n",
    "    \n",
    "    print(f'writing {output_file}...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cli_read_zips([e.as_posix() for e in zip_files] + '-o o.parquet'.split())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updates"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Core function to download new data and updated existing local structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def update(\n",
    "    kind: str, # One of 'outsiders', 'insiders', or 'minors'\n",
    "    output_directory: str | pathlib.Path # The path where hosting\n",
    "    ):\n",
    "    \"Update local data structures\"\n",
    "\n",
    "    # `kind` should be one of the pre-set types\n",
    "    assert kind in sproc.structure.tables\n",
    "\n",
    "    # just in case\n",
    "    output_directory = pathlib.Path(output_directory)\n",
    "\n",
    "    # the output directory is expected to exist\n",
    "    assert output_directory.exists()\n",
    "\n",
    "    # the name of the output file is determined by `kind`, and it's a parquet file\n",
    "    output_file = pathlib.Path(output_directory / kind).with_suffix('.parquet')\n",
    "\n",
    "    # if a there is a previous file...\n",
    "    if output_file.exists():\n",
    "\n",
    "        print(f'found previous \"{output_file}\": extending it...')\n",
    "\n",
    "        # the latter is read\n",
    "        df = pd.read_parquet(output_file)\n",
    "\n",
    "        # date strings are extracted from the \"zip\" index (level 0)...\n",
    "        date_strs = df.index.get_level_values(0).drop_duplicates().str.extract('.*_([0-9]*).zip')[0].astype('str')\n",
    "\n",
    "        # ...and parsed\n",
    "        date_strs = date_strs.apply(sproc.parse.year_and_maybe_month)\n",
    "\n",
    "        # the date from which to download new data is taken to be the maximum\n",
    "        from_date = date_strs.max()\n",
    "\n",
    "        # print(from_date)\n",
    "\n",
    "        # print(sproc.download.make_urls(**sproc.structure.tables[kind], from_date=from_date))\n",
    "\n",
    "        # required files are downloaded\n",
    "        downloaded_files = sproc.download.from_date(kind, date=from_date, output_directory=output_directory)\n",
    "\n",
    "        if not downloaded_files:\n",
    "\n",
    "            print('file is up-to-date')\n",
    "\n",
    "            return\n",
    "\n",
    "        # in the beginning, the file to be updated represents the whole history\n",
    "        history_df = df\n",
    "\n",
    "        # every file that has been downloaded...\n",
    "        for f in tqdm(downloaded_files, desc='Updatingtqdm'):\n",
    "\n",
    "            tqdm.write(f'Appending \"{f.name}\"')\n",
    "\n",
    "            # ...is used to extend the past\n",
    "            history_df = sproc.extend.df_with_zip(history_df, f)\n",
    "\n",
    "    # if a there is NOT a previous file...\n",
    "    else:\n",
    "\n",
    "        # agreed upon\n",
    "        from_date = datetime.datetime(2017, 12, 1)\n",
    "\n",
    "        print(f'no previous \"{output_file}\" was found: making one using data since {from_date.date()}...')\n",
    "\n",
    "        # print(sproc.download.make_urls(**sproc.structure.tables[kind], from_date=from_date))\n",
    "\n",
    "        # downloading\n",
    "        downloaded_files = sproc.download.from_date(kind, date=from_date, output_directory=output_directory)\n",
    "\n",
    "        # assembling\n",
    "        history_df = read_zips(downloaded_files)\n",
    "\n",
    "    # tidy up the `DataFrame` so that it can be saved in a parquet file\n",
    "    parquet_df = sproc.assemble.parquet_amenable(history_df)\n",
    "    \n",
    "    # parquet_df.to_parquet(output_file.with_stem('new'))\n",
    "    parquet_df.to_parquet(output_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A companion function to allow using the above from the command-line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def cli_update(args: list = None) -> None:\n",
    "    \n",
    "    parser = argparse.ArgumentParser(description='Update (or make) local data')\n",
    "\n",
    "    parser.add_argument('kind', choices=sproc.structure.tables.keys())\n",
    "    parser.add_argument('-o', '--output_directory', help='Output directory', default=pathlib.Path.cwd(), type=pathlib.Path)\n",
    "\n",
    "    command_line_arguments = parser.parse_args(args)\n",
    "\n",
    "    # directory is made if it doesn't exist\n",
    "    command_line_arguments.output_directory.mkdir(exist_ok=True)\n",
    "\n",
    "    print(command_line_arguments)\n",
    "\n",
    "    update(command_line_arguments.kind, command_line_arguments.output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_directory = pathlib.Path.cwd().parent / 'data' / 'plataforma'\n",
    "# args = ['outsiders', '-o', output_directory.as_posix()]\n",
    "# cli_update(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.doclinks import nbdev_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "nbdev_export('00_core.ipynb')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sproc",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
